{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical statistical inference: MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "- III. [Maximum Likelihood (MLE)](#III)\n",
    "    - III.1 [Likelihood function](#III.1-Likelihood:)\n",
    "    - III.2 [Maximum Likelihood estimation](#III.2--Maximum-Likelihood-Estimation-(MLE))\n",
    "    - III.3 [Properties of MLE estimators](#III.3.-Properties-of-Maximum-likelihood-estimators)\n",
    "    - III.4.[Illustration of the MLE approach](#III.4.-Illustration-of-the-MLE-approach)\n",
    "- IV. Regression and Model fitting: See [Regression.ipynb]([Regression.ipynb])\n",
    "- XX. [References and supplementary material](#X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Maximum Likelihood <a class=\"anchor\" id=\"III\"></a>\n",
    "\n",
    "A common way, in the frequentist sense, to make a point estimation and derive a confidence interval, it through Maximum Likelihood analysis. \n",
    "\n",
    "The first thing to do in that approach is to choose a model $M(\\theta)$ that is supposed to describe the underlying population from which the data are drawn. This allows one to calculate a likelihood as explained below. \n",
    "\n",
    "### III.1 Likelihood:\n",
    "\n",
    "\n",
    "The Likelihood $L$ of a model and of its parameters is defined as $L~=~p(D \\,|\\,M(\\boldsymbol{\\theta}))$. Hence, this is the probability of Data given a model (in fact, given a model and its associated set of parameters). \n",
    "When we talk about the likelihoods of different models we are in general talking about the likelihoods of different sets of parameter values. \n",
    "\n",
    "It is important to note that a likelihood is not strictly speaking a probability as the sum of all the possible outcomes of a model must sum up to one, while the sum of the likelihood of the models (parameters) needed to explain the data do not have to add up to 1. This can be illustrated using a Gaussian case.   \n",
    "\n",
    "Let's imagine that we have an ensemble of $N$ *independent* random variable {$x_i$} drawn from a normal (i.e. gaussian) distribution of mean $\\mu$ and width $\\sigma$ (i.e. errors on all the measurements $x_i$ are the same, namely the errors are \"homoscedastic\"). \n",
    "\n",
    "We know that the probability of a given measurement $x_k$ is:\n",
    "\n",
    "$$\n",
    "p(x_k \\, | \\, \\mu, \\sigma ) = \\frac{1}{\\sigma \\sqrt{2\\,\\pi}} \\, \\exp\\left[-0.5 \\left (\\frac{x_k - \\mu}{\\sigma} \\right)^2\\right] \n",
    "$$\n",
    "\n",
    "If each measurement is independent of the other, the probability of having a given set of measurements will be proportional to the product of the individual probabilities. The likelihood can then be calculated as:\n",
    "\n",
    "$$\n",
    "L \\equiv p(\\{x_i\\} | \\mu, \\sigma ) = \\prod_{i=1}^{N} \\frac{1}{\\sigma \\sqrt{2\\,\\pi}} \\, \\exp\\left[-0.5 \\left (\\frac{x_i - \\mu}{\\sigma} \\right)^2\\right] = \\frac{1}{\\sigma^n (2\\,\\pi)^{n/2}} \\prod_{i=1}^{N} \\exp \\left (\\frac{-(x_i - \\mu)^2}{2\\, \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Note that this can be re-written:\n",
    "\n",
    "$$\n",
    "L \\equiv p(\\{x_i\\} | \\mu, \\sigma ) = \\frac{1}{\\sigma^n (2\\,\\pi)^{n/2}} \\exp \\left ( \\sum_{i=1}^N \\frac{-(x_i - \\mu)^2}{2\\, \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "If you make a measurement of a random variable and obtain a value $x$, the associated likelihood $L$ can be considered as a function of the data (i.e. the likelihood depends on the data you have obtained). Conversely, you may also consider that you can vary model parameters, and search for the parameters that maximize the likelihood (i.e. maximize the change you have to make that specific measurement). In that case we can say that it is a function of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2  Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "The so called *Maximum Likelihood Approach* used by frequentists (and originally introduced by Fisher -as a third year undegraduate- about 100 years ago), consists in considering the likelihood as a function of the model (parameters) and not as a function of the data. The idea behind the maximization of the likelihood is two fold: i) the probability of any given set of parameters $\\boldsymbol{\\theta}$ is proportional to the probability of observing the data (i.e. the likelihood defined above); ii) the most probable set of values of the parameters (of our model) should also be the one that maximizes the probability of obtaining the data. \n",
    "\n",
    "Seeing the likelihood in the context of the Bayes theorem makes the above argument clearer. The probability to obtain a given set of parameters $\\boldsymbol{\\theta}$ : \n",
    "\n",
    "$$\n",
    "p(M({\\boldsymbol \\theta})\\,|\\,D) = \\frac{p(D \\, | \\,M({\\boldsymbol\\theta})) \\, p(M({\\boldsymbol \\theta}))) }{p(D)}\n",
    "$$\n",
    "\n",
    "[Note that in that expression $p(M({\\boldsymbol \\theta})\\,|\\,D)$ is called *posterior probability* (of the model given the data), $p(M({\\boldsymbol \\theta}))$ is the prior on the model parameters, and $p(D) = \\int \\, p(D \\, | \\, M({\\boldsymbol\\theta^\\prime}))\\, p(M({\\boldsymbol \\theta^\\prime})) {\\rm d}{\\boldsymbol \\theta}^\\prime$ is a normalisation factor. ]\n",
    "\n",
    "If you assume that you have the same probability for your parameters to occur (i.e. $p(M({\\bf \\theta}))$ is uniform), then by maximizing the likelihood, you also maximize the posterior probability of your model (parameters). \n",
    "\n",
    "Because the likelihood can quickly become very small, one generally calculates its logarithm.  The maximum of $\\ln{L}$ (varying the parameters $\\theta$) is obtained by searching the parameters $\\theta$ that yield:\n",
    "\n",
    "$$\n",
    "\\left. \\frac{{\\rm d}ln(L(\\theta_i))}{\\rm{d}\\theta_i}\\right\\vert_{\\hat {\\theta_i}} \\equiv 0\n",
    "$$\n",
    "\n",
    "For our gaussian example, and specialised to the mean $\\mu$, we have:\n",
    "\n",
    "$$\n",
    "\\ln(L) =  -\\frac{N}{2}\\,\\ln(2\\pi) - N\\,\\ln(\\sigma)  - \\sum_{i=1}^N \\frac{(x_i \\,- \\, \\mu)^2}{2\\,\\sigma^2}, \n",
    "$$\n",
    "\n",
    "which is maximum for:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{N} \\, \\sum_{i=1}^{N} x_i\n",
    "$$\n",
    "\n",
    "We can also calculate the derivative w.r.t. to $\\sigma$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial ln(L)}{\\partial \\sigma} = 0 = -\\frac{N}{\\sigma} \\, +\\, \\frac{1}{\\sigma^3} \\, \\sum_i \\, (x_i - \\mu)^2 \n",
    "$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{N} \\, \\sum_i{(x_i-\\mu)^2}\n",
    "$$\n",
    "\n",
    "This tells us that our estimator of the mean, $\\hat{\\mu}$, and of the variance (for a distribution of known mean) $\\hat{\\sigma}^2$ are maximum likelihood estimators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III.2.1 MLE applied to heteroscedastic errors:   <a class=\"anchor\" id=\"III.2.1\"></a>\n",
    "\n",
    "We'll see here what is the maximum likelihood estimator for a sample of data points drawn from a gaussian distribution but each point having a different error $\\sigma_i$ (heteroscedastic errors).  \n",
    "\n",
    "The approach is the same as for homoscedastic errors but replacing $\\sigma$ by $\\sigma_i$.    \n",
    "The likelihood is the product of the $p(x \\, | \\, \\mu, \\sigma_i)$ associated to each points.   \n",
    "\n",
    "$$\n",
    "L = \\prod_{i=1}^{N} \\frac{1}{\\sigma_i \\sqrt{2\\,\\pi}} \\, \\exp\\left[-0.5 \\left (\\frac{x_i - \\mu}{\\sigma_i} \\right)^2\\right] =  \\frac{1}{(2\\,\\pi)^{n/2}} \\prod_{i=1}^{N} \\frac{1}{\\sigma_i} \\exp \\left (\\frac{(x_i - \\mu)^2}{2\\, \\sigma_i^2} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We therefore have :\n",
    "\n",
    "$$\n",
    "\\ln(L) =  -\\frac{N}{2}\\,\\ln(2\\pi) - \\sum_i \\ln(\\sigma_i)  - \\sum_{i=1}^N \\frac{(x_i \\,- \\, \\mu)^2}{2\\,\\sigma_i^2}, \n",
    "$$\n",
    "\n",
    "So, taking the partial derivative of this likelihood w.r.t. $\\mu$, we get:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{\\sum_i \\, w_i \\, x_i}{\\sum_i \\, w_i}\n",
    "$$\n",
    "\n",
    "where $w_i = \\sigma^{-2}_i$ . \n",
    "\n",
    "Hence the MLE estimator of the mean is simply the weighted arithmetic mean of all measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3. Properties of Maximum likelihood estimators\n",
    "\n",
    "MLE are popular in frequentist inference because they gather important properties:\n",
    "\n",
    "1. **consistency**: This means that they converge to the true value when the size of the sample increases. \n",
    "2. **asymptotically normal** estimator: When the size of the sample increases, the distribution of the parameter estimator $\\hat{\\theta}$ converges towards a normal distribution centered on the MLE, with a certain spread. This spread can often be calculated easily to estimate the uncertainty on $\\hat{\\theta}$.\n",
    "3. **best possible error** given the data. This means that asymptotically, MLE achieve the **theoretical possible minimum of the variance** (Cramèr-Rao bound). \n",
    "4. **Equivariant** estimator: if $\\hat{\\theta}$ is a MLE estimator of $\\theta$, then $g(\\hat{\\theta})$ is a MLE of $g(\\theta)$\n",
    "\n",
    "**Warning:** these properties rely on some assumptions regarding i) the model from which are drawn the data (i.e. they are all drawn from the same class of pdf -a gaussian in the model above); ii) regularity conditions that allows some derivative to exists (see Chapter 10 of [Lupton 1993](#LUP93) ). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.4. Illustration of the MLE approach \n",
    "\n",
    "Let's first look to a simple illustration that consists of a Poisson distribution. \n",
    "First, we define the log-likelihood. Then we make some Poisson sample data, with a rate parameter of 10. This could be, for example, a count rate in a photon detector detecting of the order of 10 photons per second. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = np.random.poisson(10, size=10000)\n",
    "print(sample[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_loglike(rate, data):\n",
    "    ## make rate parameter into array of same length as data:\n",
    "    r = np.ones(len(data))*rate\n",
    "    ## now we can compute the log-likelihood:\n",
    "    llike = -np.sum(r) + np.sum(data*np.log(r))-np.sum(scipy.special.gammaln(data + 1))\n",
    "    return llike\n",
    "\n",
    "np.random.seed(123546)\n",
    "sample = np.random.poisson(10, size=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll leave it to you to verify that the expression in the definition of the likelihood function is indeed that of the Poisson likelihood for N samples. \n",
    "\n",
    "Now, let's have a look at the shape of the likelihood. We'll use the first 10, 100, 1000, 5000 events from our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## guess_like returns likelihood associated to a sample s, for a bunch of guessed values for the rate \n",
    "# (in the range 0, 100)\n",
    "def guess_like(s):\n",
    "    guesses = np.arange(100)\n",
    "    like_all = [poisson_loglike(g, s) for g in guesses]  \n",
    "    return like_all\n",
    "\n",
    "def plot_like(nsamples, color):\n",
    "    np.random.seed(123546)\n",
    "    sample = np.random.poisson(10, size=10000)\n",
    "    x = np.arange(100)\n",
    "    llike = np.array(guess_like(sample[:nsamples]))\n",
    "    plt.plot(x, llike, label=\"%i samples\"%nsamples, color=color)\n",
    "    lpos = np.array(guess_like(sample[:nsamples])).argmax()\n",
    "    return  llike, x[lpos]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,4))\n",
    "colors = ['blue', 'red', 'pink', 'black']\n",
    "for j, nsamp in enumerate([10, 100, 1000, 5000]):   # \"Number of measurements I take\"\n",
    "    log_L, x = plot_like(nsamp, colors[j])\n",
    "    print('Max likelihood log(L)=%.2f, from %.i points, at %.2f vs E(X) %.2f' %(log_L.max(), nsamp, x, np.mean(sample[:nsamp])))\n",
    "plt.vlines(10, 0, -100000)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative is to use minimization algorithm such as e.g. powell function minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsamples = 5000\n",
    "## define the -log-likelihood as an \"anonymous function\" lambda\n",
    "neg_poisson = lambda r: -poisson_loglike(r, sample[:nsamples])\n",
    "\n",
    "## minimise using Powell's algorithm, starting value for rate parameter is 20\n",
    "result = scipy.optimize.fmin_powell(neg_poisson, 20, full_output=True)\n",
    "\n",
    "print(\"The fit-parameter for the Poisson rate parameter: %.3f \"%result[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX. References and supplementary material: <a class=\"anchor\" id=\"X\"></a>\n",
    "\n",
    "**Chapter 4** (4.5, 4.7), **Chapter 8** (8.1, 8.2, 8.7) of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy. \n",
    "\n",
    "* $\\chi^2$ and linear fits, Andy Gould, https://arxiv.org/abs/astro-ph/0310577\n",
    "\n",
    "* *All of statistics: a concise course in statistical inference*, Wasserman 2004  <a class=\"anchor\" id=\"WAS04\"></a> (see also errata in http://www.stat.cmu.edu/~larry/all-of-statistics/): **Chapter 8, 9 **\n",
    "\n",
    "* *Statistics in theory and Practice*, Lupton 1993 <a class=\"anchor\" id=\"LUP93\"></a>: **Chapter 6, 7, 8, 9 **\n",
    "\n",
    "* [Numerical recipes](http://www2.units.it/ipl/students_area/imm2/files/Numerical_Recipes.pdf) by Press et al. Cambridge University press: **Chapter 15**\n",
    "\n",
    "Other useful references to know more about the topics covered in this class: \n",
    "\n",
    "- Sklearn help: http://scikit-learn.org/stable/modules/linear_model.html\n",
    "\n",
    "- Guide on General Least Square regression by Paul Johnson: http://pj.freefaculty.org/guides/stat/Regression/GLS/GLS-1-guide.pdf\n",
    "\n",
    "- Contribution of Fisher to MLE: J.A. Aldrich R. A. Fisher and the Making of Maximum Likelihood 1912– 1922 About Fisher's invention of Maximum Likelihood: Statistical science, 1997, 12, 3, 162 https://projecteuclid.org/download/pdf_1/euclid.ss/1030037906 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel-python2]",
   "language": "python",
   "name": "conda-env-intel-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
