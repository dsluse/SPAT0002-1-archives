{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4190fd9d-b489-49cf-af98-dc01233625ea"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian inference: MCMC\n",
    "\n",
    "## Table of Content:\n",
    "\n",
    "- I-II-III Basic concepts of Bayesian inference: See [Bayes_basics.ipynb](Bayes_basics.ipynb)\n",
    "- IV Simple Bayesian Modeling: See [Bayes_simple_modeling.ipynb](Bayes_simple_modeling.ipynb)\n",
    "- V Bayesian modeling with [Monte Carlo Markov Chains (MCMC)](#V.-Bayesian-Modeling-with-MCMC)\n",
    "    * V.1 [The Curse of dimensionality](#V.1-The-Curse-of-dimensionality)\n",
    "    * V.2 [Circumventing the curse with sampling](#V.2-Circumventing-the-Curse-with-Sampling)\n",
    "- X [References](#XX-References:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "50db2b1d-1e2e-4227-952e-51be59d35c28"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## V. Bayesian Modeling with MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7d08431d-77a2-4bbd-bcec-8318f0e24df5"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the previous section we explored a Bayesian solution to a straight line fit.\n",
    "The result made use of the evaluation of a posterior across a grid of parameters: a strategy that *will not* scale to higher-dimensional models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d04dedeb-0437-4d2c-b95e-5d2a88324686"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## V.1 The Curse of dimensionality\n",
    "\n",
    "The reason it will not scale is one of the effects of the ubiquitous \"Curse of Dimensionality\". To understand this, consider how many evaluations we need for an $N$-dimensional grid with 100 samples per dimension\n",
    "\n",
    "In one dimension, we have $100$ points.\n",
    "\n",
    "In two dimensions we have $100^2 = 10,000$ evaluations.\n",
    "\n",
    "In three dimensions, we have $100^3 = 1,000,000$ evaluations.\n",
    "\n",
    "In $N$ dimensions, we have $100^N$ evaluations, and as $N$ grows this quickly becomes untenable! For example, if we have only six model parameters, this \"dense grid\" approach will require evaluating the posterior at one trillion grid points, the results of which would require several terabytes of memory just to store!\n",
    "\n",
    "Evidently the dense grid strategy will not work for any but the simplest Bayesian models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "70cc1ba0-389c-40bf-b24b-4abaad71644d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## V.2 Circumventing the Curse with Sampling\n",
    "\n",
    "An idea that revolutionized Bayesian modeling (and made possible the wide variety of Bayesian approaches used in practice today) is *Markov Chain Monte Carlo* (MCMC), an approach that allows one to efficiently draw (pseudo)random samples from a posterior distribution even in relatively high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e4194822-15d8-4b4f-83e7-a20407989822"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### V.2.1 The Metropolis-Hastings Sampler\n",
    "\n",
    "Perhaps the simplest of MCMC samplers is the *Metropolis-Hastings Sampler*.\n",
    "This provides a procedure which, given a pseudo-random number generator, selects a chain of points which (in the long-term limit) will be a representative sample from the posterior. The procedure is surprisingly simple:\n",
    "\n",
    "1. Define a posterior $p(\\theta~|~D, I)$\n",
    "2. Define a *proposal density* $p(\\theta_{i + 1}~|~\\theta_i)$, which must be a symmetric function, but otherwise is unconstrained (a Gaussian is the usual choice). This defines the way you fix the next step. \n",
    "3. Choose a starting point $\\theta_0$\n",
    "4. Repeat the following:\n",
    "\n",
    "   1. Given $\\theta_i$, draw a new $\\theta_{i + 1}$\n",
    "   \n",
    "   2. Compute the *acceptance ratio*\n",
    "      $$\n",
    "      a = \\frac{p(\\theta_{i + 1}~|~D,I)}{p(\\theta_i~|~D,I)}\n",
    "      $$\n",
    "   \n",
    "   3. If $a \\ge 1$, the proposal is more likely: accept the draw and add $\\theta_{i + 1}$ to the chain.\n",
    "   \n",
    "   4. If $a < 1$, then accept the point with probability $a$: this can be done by drawing a uniform random number $r$ and checking if $a < r$. If the point is accepted, add $\\theta_{i + 1}$ to the chain. If not, then add $\\theta_i$ to the chain *again*.\n",
    "\n",
    "There are a few caveats to be aware of when using MCMC\n",
    "\n",
    "#### 1. The procedure is provably correct... but only in the long-term limit!\n",
    "\n",
    "Sometimes the long-term limit is **very** long. What we're looking for is \"stabilization\" of the MCMC chain, meaning that it has reached a statistical equilibrium. There is a vast literature on how to measure stabilization of an MCMC chain. Here we'll use the sloppy but intuitive LAI approach (i.e. Look At It).\n",
    "\n",
    "#### 2. The size of the proposal distribution is *very* important\n",
    "\n",
    "- If your proposal distribution is too small, it will take too long for your chain to move, and you have the danger of getting stuck in a local maximum for a long (but not infinite) time.\n",
    "\n",
    "- If your proposal distribution is too large, you will not be able to efficiently explore the space around a particular peak\n",
    "\n",
    "In general, choosing an appropriate scale for the proposal distribution is one of the most difficult parts of using the MCMC procedure above.\n",
    "More sophisticated methods (see later) have built-in ways to estimate this along the way, but it's still something to be aware of!\n",
    "\n",
    "#### 3. Fast Stabilization can be helped by good initialization\n",
    "\n",
    "In practice, assuring that MCMC will stabilize quickly has a lot to do with choosing a suitable initialization. For this purpose, it can be useful to find the maximum a posteriori (MAP) value, and initialize the chain with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6bac120d-447c-4bd7-a053-d4b29bc56188"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### V.2.2 Sampling with ``emcee``\n",
    "\n",
    "There are several good Python approaches to Bayesian computation with MCMC. \n",
    "Here we'll focus on [``emcee``](http://dan.iel.fm/emcee/), a lightweight Python package developed by Dan Foreman-Mackey and collaborators.\n",
    "One benefit of ``emcee`` is that it uses an *ensemble sampler* which automatically tunes the shape and size of the proposal distribution (you can read more details in the ``emcee`` documentation).\n",
    "\n",
    "Let's apply MCMC to our simple line fitting problem. The following steps are required: \n",
    "\n",
    "#### V2.2.1 Expression of the posterior (likelihood and prior)\n",
    "\n",
    "For that purpose, we need first to define a function that enables us to evaluate the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2a363779-cb55-4905-8ffc-b7d001d41e92"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "69363a8e-a4d2-41cb-8abb-9dd678c43fd8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Function that generates points following a linear trend. \n",
    "def make_data(intercept, slope, N=20, dy=5, rseed=42):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------\n",
    "    intercept, slope: parameters of the linear model\n",
    "    dy: width of normally distributed data points around the \"true line\"\n",
    "    rseed: Seed of the random number generator (fixed for the sake of discussion)\n",
    "    Output:\n",
    "    -------\n",
    "    x, y, sig_y \n",
    "    '''\n",
    "    rand = np.random.RandomState(rseed)\n",
    "    x = 100 * rand.rand(N)\n",
    "    y = intercept + slope * x\n",
    "    y += dy * rand.randn(N)\n",
    "    return x, y, dy * np.ones_like(x)\n",
    "\n",
    "theta_true = [25, 0.5]\n",
    "x, y, dy = make_data(theta_true[0], theta_true[1])  # could also be make_data(*theta_true)\n",
    "plt.errorbar(x, y, dy, fmt='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "93d140a4-ca2e-43b6-846a-f71c7e0699b4"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Likelihood and Prior (cf Bayes_simple_modeling.ipynb)\n",
    "def ln_likelihood(theta, x, y, dy):\n",
    "    y_model = theta[0] + theta[1] * x\n",
    "    return -0.5 * np.sum(np.log(2 * np.pi * dy ** 2) + (y - y_model) ** 2 / dy ** 2)\n",
    "\n",
    "def ln_flat_prior(theta):\n",
    "    '''\n",
    "    theta = shape(2) array containing the [intercept, slope] = [theta_0, theta_1]\n",
    "    '''\n",
    "    if np.all(np.abs(theta[1]) < 1000):\n",
    "        return 0 # log(1)\n",
    "    else:\n",
    "        return -np.inf  # log(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "d95394fe-1b40-4bb7-b49b-3efac767df3c"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def ln_posterior(theta, x, y, dy):\n",
    "    return ln_flat_prior(theta) + ln_likelihood(theta, x, y, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0646f72-902c-4ad8-bdab-c010c47aa512"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### V.2.2.2 Using emcee to sample the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install emcee   # If emcee is not installed on your machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "a46f9dca-ec80-4385-a414-4f6a618e26aa"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "import importlib\n",
    "importlib.reload(emcee)\n",
    "\n",
    "ndim = 2  # number of parameters in the model\n",
    "nwalkers = 50  # number of MCMC walkers\n",
    "#sampler.reset()  # Uncomment if you start over, ignoring what you did before\n",
    "# initialize walkers ; You can make a more refined start ! Closer to expected value parameters ... , e.g. based on MLE ...\n",
    "starting_guesses = np.random.randn(nwalkers, ndim)\n",
    "\n",
    "# Set up the sampler\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_posterior, args=[x, y, dy])  # This sets up the posterior\n",
    "\n",
    "# starting_guesses = The initial position vector.  \n",
    "# Can also be None to resume from where :func:``run_mcmc`` left off the last time it executed.\n",
    "# param1 = # of steps to run (here: 200)\n",
    "pos, prob, state = sampler.run_mcmc(starting_guesses, 200)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8982a46e-4005-4c06-a72f-3d33794c0605"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sampler.chain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "0f60ea88-5ee3-47fc-84e8-50f1da260114"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "starting_guesses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8642a84d-b578-4cde-9b88-b22e9a3c0fce"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(ncols=2, figsize=(12,4))\n",
    "ax[0].plot(starting_guesses[:,0], starting_guesses[:,1], ls='', marker='.', label='Start')\n",
    "ax[0].set_title('Starting positions')\n",
    "ax[0].plot(sampler.chain[:, 0, 0], sampler.chain[:, 0, 1], ls='', marker='d', alpha=0.4, label='Step 0')\n",
    "ax[0].plot(sampler.chain[:, 1, 0], sampler.chain[:, 1, 1], ls='', color='red', marker='s', alpha=0.2, label='Step 1')\n",
    "ax[0].legend()\n",
    "ax[1].plot(pos[:,0], pos[:,1], ls='', marker='.')\n",
    "ax[1].set_title('Final positions')\n",
    "ax[1].plot(sampler.chain[:, 199, 0], sampler.chain[:, 199, 1], ls='', marker='d', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a53441d9-bcd8-4365-aaae-822138c64215"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### V.2.2.3 Plotting the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9313aff7-18da-42f8-a550-67c649b0295c"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "for i in range(2):\n",
    "    ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0d798fd9-6279-4e85-9d39-76208264a5fb"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### V.2.2.4 Restarting after burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7d4efc30-e0f9-418b-80f2-2da73508b08d"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sampler.reset()\n",
    "pos, prob, state = sampler.run_mcmc(pos, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "for i in range(2):\n",
    "    ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "03aa384e-e7d7-49f6-8d7b-89b8387b0f18"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Using the [corner.py](https://pypi.python.org/pypi/corner) package, we can take a look at this multi-dimensional posterior, along with the input values for the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "c037aeb3-f34e-450b-9b3d-d2ee60f7abbd"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "51bedfe9-b0e6-4404-be58-135a5e3a8fbe"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "qq = corner.corner(sampler.flatchain, labels=['intercept', 'slope'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "e3260b3c-02eb-4578-b372-c9789e3aadde"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# If corner is not installed, you can simply do the following\n",
    "from scipy.stats import gaussian_kde\n",
    "#t0, t1 = sampler.flatchain[:,0], sampler.flatchain[:,1]   # full chain ... a bit expensive for calculating the density of points\n",
    "rd = np.random.choice(len(sampler.flatchain[:,0]), 1000)  # I pick 1000 random pos in my chain\n",
    "t0 = [sampler.flatchain[i,0] for i in rd]\n",
    "t1 = [sampler.flatchain[i,1] for i in rd]\n",
    "# Calculate the point density\n",
    "t01 = np.vstack([t0,t1])\n",
    "density = gaussian_kde(t01)(t01)\n",
    "\n",
    "f, ax = plt.subplots(ncols=3, figsize=(12,4))\n",
    "ax[0].scatter(t0, t1, c=density)\n",
    "h1 = ax[1].hist(t0, bins=20, normed=True)\n",
    "h2 = ax[2].hist(t1, bins=20, normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "57d6a3ff-8227-4b78-89cb-173f5fd7ab49"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### V.2.2.5 Visualizing the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2936f1b1-1a84-4d65-b4ed-ba7d4b2b0a66"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A way to visualize the posterior is to plot the model over the data.\n",
    "Each point in the two-dimensional space ($\\theta_0$,$\\theta_1$) explored by the sampler, corresponds to a possible model for our data. If we select ~100 of these at random and plot them over our data, it will give us a good idea of the spread in the model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "41613d42-3d00-4b45-ac55-024c1415e67c"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "chain = sampler.flatchain\n",
    "chain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "244b081a-ccc2-4743-8898-0eaa58ec39fd"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 100)\n",
    "thetas = [chain[i] for i in np.random.choice(chain.shape[0], 100)]  # you randomly pick 100 pairs of parameters from the chain\n",
    "\n",
    "for i in range(100):\n",
    "    theta = thetas[i]\n",
    "    plt.plot(xfit, theta[0] + theta[1] * xfit, color='black', alpha=0.05)\n",
    "\n",
    "plt.errorbar(x, y, dy, fmt='o');\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c79dceef-3639-4ef9-890e-357307ee6d95"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### V.2.3. Fitting a straight line with intrinsic scatter\n",
    "\n",
    "Above we have done a simple model, where the data is drawn from a straight line.\n",
    "\n",
    "Often, however, we will be modeling relationships where there is some intrinsic scatter in the model itself: that is, even if the data were *perfectly* measured, they would not fall along a perfect straight line, but would have some (unknown) scatter about that line.\n",
    "\n",
    "Here we'll make a slightly more complicated model in which we will fit for the slope, intercept, and intrinsic scatter (i.e. intrinsic scatter is a parameter of our model) all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "159b3510-7e2f-4074-9e52-55a4d9401b73"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def make_data_scatter(intercept, slope, scatter, N=20, dy=2, rseed=42):\n",
    "    rand = np.random.RandomState(rseed)\n",
    "    x = 100 * rand.rand(20)\n",
    "    y = intercept + slope * x\n",
    "    y += np.sqrt(dy ** 2 + scatter ** 2) * rand.randn(20)\n",
    "    return x, y, dy * np.ones_like(x)\n",
    "\n",
    "theta = (25, 0.5, 3.0)  # (intercept, slope, intrinsic scatter)\n",
    "x, y, dy = make_data_scatter(*theta)\n",
    "plt.errorbar(x, y, dy, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cf275c2b-7d4e-465c-a166-e492b79b5eb5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now go through the different steps to solve this problem in a Bayesian way, and use MCMC to evaluate the posterior on the parameters. \n",
    "\n",
    "#### V.2.3.1 Defining the likelihood and prior\n",
    "\n",
    "You are now getting familiar with the first step of the procedure which consists in deriving an expression for the likelihood. The likelihood for this model looks very similar to what we used above, except that the intrinsic scatter is added *in quadrature* to the measurement error.\n",
    "\n",
    "\n",
    "$$\n",
    "P(D\\mid\\boldsymbol{\\theta}) =  \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\,\\pi (\\sigma_i^2 + \\sigma^2)}} \\, \\exp\\left[\\left (\\frac{ -(y_i - (\\theta_0+\\theta_1\\,x_i))^2}{2\\,(\\sigma_i^2+\\sigma^2)} \\right)\\right]\n",
    "$$\n",
    "\n",
    "For the prior, you can use either a flat or symmetric prior on the slope and intercept, but on the intrinsic scatter $\\sigma$ it is best to use a scale-invariant Jeffreys Prior:\n",
    "\n",
    "$$\n",
    "P(\\sigma)\\propto\\sigma^{-1}\n",
    "$$\n",
    "\n",
    "As discussed before, this has the nice feature that the resulting posterior will not depend on the units of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f105c475-0451-40f7-a197-7db542ab6c38"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec52b908-90ad-439d-9eb9-05b34b94dee2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "f9e4ba9f-5889-4657-a9f0-e4b7da2bf598"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Define functions to compute the log-prior, log-likelihood, and log-posterior\n",
    "\n",
    "# theta = [intercept, slope, scatter]\n",
    "\n",
    "def ln_prior(theta):\n",
    "    # fill this in\n",
    "    return\n",
    "    \n",
    "def ln_likelihood(theta, x, y, dy):\n",
    "    # fill this in\n",
    "    return\n",
    "\n",
    "def ln_posterior(theta, x, y, dy):\n",
    "    # fill this in\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6e170798-2a4e-4b83-b40c-1473c03f3524"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Sampling from the Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "0d947a5c-7c97-47a1-a78d-d92363a66cd3"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Using emcee, create and initialize a sampler and draw 200 samples from the posterior.\n",
    "# Remember to think about what starting guesses should you use!\n",
    "# You can use the above as a template\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "729ae720-f1d2-46e0-b36c-11799e2a18f4"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Visualizing the Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "ce2dbf0c-965c-465b-a8a0-dec04cfe6053"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the three chains as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "48d7a4bb-d022-4c16-a03a-d14f0cf4ee83"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Resetting and getting a clean sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "a065637b-db3d-47ce-8083-82a1e0ab6499"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Are your chains stabilized? Reset them and get a clean sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9cfadfcc-1b95-4944-979b-e27d684548b1"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "95c9d5ac-8535-4196-a6de-b8342d9cc946"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Use e.g. corner.py to visualize the three-dimensional posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "9b3f9b46-f8f8-4cbf-838a-54b2f540daca"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Next plot ~100 of the samples as models over the data to get an idea of the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "bde6af9d-fed6-47d5-a939-2228fdfa2909"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## XX References:\n",
    "\n",
    "**Chapter 5** (5.1, 5.2, 5.3, 5.8) of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy. \n",
    "\n",
    "- This notebook includes a large fraction of the material that J. Vander Plas gave during the \"Bayesian Methods in Astronomy workshop\", presented at the 227th meeting of the American Astronomical Society. The full repository with that material can be found on GitHub: http://github.com/jakevdp/AAS227Workshop\n",
    "\n",
    "- More insights on the differences between frequentist and Bayesian approaches: see [J. VanderPlass blog posts](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) \n",
    "\n",
    "- Jayes: [*Probability Theory: The Logic of Science*](http://bayes.wustl.edu/etj/prob/book.pdf).\n",
    "\n",
    "- For some approachable reading on frequentist vs. Bayesian uncertainties, I'd suggest [The Fallacy of Placing Confidence in Confidence Intervals](https://learnbayes.org/papers/confidenceIntervalsFallacy/), as well as Jake VanderPlast blog post on the topic, [Confidence, Credibility, and why Frequentism and Science do not Mix](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/).\n",
    "\n",
    "- Foreman-Mackey et al. 2012 [*EMCEE, the MCMC hammer*](https://arxiv.org/abs/1202.3665) ; see also http://dan.iel.fm/emcee/current/\n",
    "\n",
    "- About the variety of approaches to MCMC: Allison and Dunkley 2013: [Comparison of sampling techniques for Bayesian parameter estimation](https://arxiv.org/abs/1308.2675). See also [How to Be a Bayesian in Python](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/). \n",
    "\n",
    "- Andreon 2011 [Understanding better (some) astronomical data using Bayesian methods](https://arxiv.org/abs/1112.3652)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "7901333d-491b-4c9b-9ac0-094b49746dd6"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:intel-python2]",
   "language": "python",
   "name": "conda-env-intel-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
