{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Bayesian inference: Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Implementing your own MCMC\n",
    "\n",
    "As we have seen, the standard Bayesian approach to model fitting involves sampling the posterior, usually via a variant of Markov Chain Monte Carlo (MCMC). Though there are many very sophisticated MCMC samplers out there, the most simple algorithm (Metropolis-Hastings) is rather straightforward to code.\n",
    "\n",
    "Here we'll walk through creating our own Metropolis-Hastings sampler from scratch, in order to better understand exactly what is going on under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As usual, we start with some imports\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "#from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.1 Metropolis-Hastings Procedure\n",
    "\n",
    "Recall the Metropolis-Hastings procedure:\n",
    "\n",
    "1. Define a posterior $p(\\theta~|~D, I)$\n",
    "2. Define a *proposal density* $p(\\theta_{i + 1}~|~\\theta_i)$, which must be a symmetric function, but otherwise is unconstrained (a Gaussian is the usual choice).\n",
    "3. Choose a starting point $\\theta_0$\n",
    "4. Repeat the following:\n",
    "\n",
    "   1. Given $\\theta_i$, draw a new $\\theta_{i + 1}$ from the proposal distribution\n",
    "   \n",
    "   2. Compute the *acceptance ratio*\n",
    "      $$\n",
    "      a = \\frac{p(\\theta_{i + 1}~|~D,I)}{p(\\theta_i~|~D,I)}\n",
    "      $$\n",
    "   \n",
    "   3. If $a \\ge 1$, the proposal is more likely: accept the draw and add $\\theta_{i + 1}$ to the chain.\n",
    "   \n",
    "   4. If $a < 1$, then accept the point with probability $a$: this can be done by drawing a uniform random number $r$ and checking if $a < r$. If the point is accepted, add $\\theta_{i + 1}$ to the chain. If not, then add $\\theta_i$ to the chain *again*.\n",
    "   \n",
    "The goal is to produce a \"chain\", i.e. a list of $\\theta$ values, where each $\\theta$ is a vector of parameters for your model.\n",
    "Here we'll write a simple Metropolis-Hastings sampler in Python.\n",
    "\n",
    "Note that the ``np.random.randn()`` function will be useful: it returns a pseudorandom value drawn from a standard normal distribution (i.e. mean of zero and variance of 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI.2 Implementation\n",
    "\n",
    "The cells below describe the various steps that you will implement by yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Data:* We'll use data drawn from a straight line model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data(intercept, slope, N=20, dy=2, rseed=42):\n",
    "    rand = np.random.RandomState(rseed)\n",
    "    x = 100 * rand.rand(20)\n",
    "    y = intercept + slope * x\n",
    "    y += dy * rand.randn(20)\n",
    "    return x, y, dy * np.ones_like(x)\n",
    "\n",
    "theta_true = (2, 0.5)\n",
    "x, y, dy = make_data(*theta_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First plot the data to see what we're looking at (Use a ``plt.errorbar()`` plot with the provided data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model will also be a line, as we have done in the previous lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(theta, x):\n",
    "    # the `theta` argument is a list of parameter values, e.g., theta = [slope, intercept] for a line\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with the assumption that the data are independent and identically distributed so that the likelihood is simply a product of Gaussians (one big Gaussian). We'll also assume that the uncertainties reported are correct, and that there are no uncertainties on the `x` data. We need to define a function that will evaluate the (ln)likelihood of the data, given a particular choice of your model parameters. A good way to structure this function is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_likelihood(theta, x, y, dy):\n",
    "    # we will pass the parameters (theta) to the model function\n",
    "    # the other arguments are the data\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about priors? Remember your prior only depends on the model parameters, but be careful about what kind of prior you are specifying for each parameter. Do we need to properly normalize the probabilities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_prior(theta):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define a function that evaluates the (ln)posterior probability, which is just the sum of the ln prior and ln likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_posterior(theta, x, y, dy):\n",
    "    return ln_prior(theta) + ln_likelihood(theta, x, y, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function to actually run a Metropolis-Hastings MCMC sampler. Ford (2005) includes a great step-by-step walkthrough of the Metropolis-Hastings algorithm, and we'll base our code on that. Fill-in the steps mentioned in the comments below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_mcmc(ln_posterior, nsteps, theta0, stepsize, args=()):\n",
    "    \"\"\"\n",
    "    Run a Markov Chain Monte Carlo\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ln_posterior: callable\n",
    "        our function to compute the posterior\n",
    "    nsteps: int\n",
    "        the number of steps in the chain\n",
    "    theta0: list\n",
    "        the starting guess for parameters theta\n",
    "    stepsize: float\n",
    "        a parameter controlling the size of the random step\n",
    "        e.g. it could be the width of the Gaussian distribution\n",
    "    args: tuple (optional)\n",
    "        additional arguments (data) passed to ln_posterior\n",
    "    \"\"\"\n",
    "    # Create the array of size (nsteps, ndims) to hold the chain\n",
    "    # Initialize the first row of this with theta0\n",
    "    \n",
    "    # Create the array of size nsteps to hold the ln-likelihoods for each point\n",
    "    # Initialize the first entry of this with the ln likelihood at theta0\n",
    "    \n",
    "    # Loop for nsteps\n",
    "    for i in range(1, nsteps):\n",
    "        # Randomly draw a new theta from the proposal distribution.\n",
    "        # for example, you can do a normally-distributed step by utilizing\n",
    "        # the scipy.random.normal() function\n",
    "        \n",
    "        # Calculate the probability for the new state\n",
    "        \n",
    "        # Compare it to the probability of the old state\n",
    "        # Using the acceptance probability function\n",
    "        # (remember that you've computed the log probability, not the probability!)\n",
    "        \n",
    "        # Chose a random number r between 0 and 1 to compare with p_accept\n",
    "        \n",
    "        # If p_accept>1 or p_accept>r, accept the step\n",
    "            # Save the position to the i^th row of the chain\n",
    "            \n",
    "            # Save the probability to the i^th entry of the array\n",
    "            \n",
    "        # Else, do not accept the step\n",
    "            # Set the position and probability are equal to the previous values\n",
    "            \n",
    "    # Return the chain and probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the MCMC code on the data provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the position of the walker as a function of step number for each of the parameters. Are the chains converged? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make histograms of the samples for each parameter. Should you include all of the samples? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To plot a 2D histogram, consider the use of plt.hist2d()  ; you can also use corner as last time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report to us your constraints on the model parameters.   \n",
    "This is the number for the abstract of the paper (or to report in the text of your thesis work) â€“ the challenge is to figure out how to accurately summarize a multi-dimensional posterior (which is **the result** in Bayesianism) with a few numbers (which is what readers want to see as they skim the paper/thesis/report).\n",
    "\n",
    "What numbers should you use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX References:\n",
    "\n",
    "**Chapter 5 ** (5.1, 5.2, 5.3, 5.8) of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy. \n",
    "\n",
    "- This notebook includes a large fraction of the material that J. Vander Plas gave during the \"Bayesian Methods in Astronomy workshop\", presented at the 227th meeting of the American Astronomical Society. The full repository with that material can be found on GitHub: http://github.com/jakevdp/AAS227Workshop\n",
    "\n",
    "- About the variety of approaches to MCMC: Allison and Dunkley 2013: [Comparison of sampling techniques for Bayesian parameter estimation](https://arxiv.org/abs/1308.2675). See also [How to Be a Bayesian in Python](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/). \n",
    "\n",
    "- Andreon 2011 [Understanding better (some) astronomical data using Bayesian methods](https://arxiv.org/abs/1112.3652)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
